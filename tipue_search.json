{
    "pages": [
        {
            "title": "MacBook Proを捨ててThinkpad X220を買った", 
            "text":"関連: MacBook Proを捨ててThinkpad T460sを買ってgentooを入れた - joker1007の日記 概要 正確にいうとMBP Late 2016を買おうと思っていたのだけど結局Thinkpad X220を買った話です 今年10月の例のイベントで発表されたMBPは本当に楽しみにしていたし、MBP費用も貯めていたのだけど やっぱり欲しくなくなってしまった理由としてAppleがもう開発者の方向を向いてないかもしれない、というのをあのイベントを見て強く感じたからです。 Appleの方向性に共感していたら買ったかもしれないけどTouchBarの方向性はちょっと支持出来なかったので… あと最近Docker for Windowsを久しぶりに使ったらmount周りもちゃんと動いたのでWindows環境でも自分の場合は普段の開発にほとんど問題なくなったというのもあります。 自分で作ったアプリは全てDockerizeするDockerオタクなのでこういうときはさすがに恩恵を感じます。 Linuxデスクトップを導入することも考えたけどドライバ周りや普段使い、会社での相互運用性を考えると常用するには少しハードルが高いなと思いWindowsにしています。 ThinkPad X220 スペック Core i5-2660M Mem: 4G Disk: HDD 480GB ディスプレイ解像度: 1366x768 MBPとの比較 ESCがデカい キーボードが最高 トラックポイントはいいぞ 2～3万で買える バッテリ持ちは悪い 1.5kgぐらいあるので若干重い パーツが大量に出回って分解も簡単なので直しやすい スタバでMacを威嚇出来る ThinkPad X220は2011年発売のモデルで、アイソレーションキーボードになる前のクラシック型のキーボードとしては最後のold ThinkPadです。 Sandy Bridge世代のCPUと高い拡張性で2016年現在でもパーツの換装を行えば十分使用可能ながら、企業からのリース品が大量に出回って中古市場で2～3万でとこなれて来てるので購入するには丁度いい頃だと思います。 Sandy Bridgeと最新のSkylake世代のCPUを比較しても性能差はほぼないのでCPUバウンドな処理を行わなければSSDとメモリ換装で最新ノートPCと比べてもほぼ遜色ないんじゃないでしょうか。 さすがにPCIe-NVMeディスクを搭載出来るような機種にはSATA SSDに換装してもディスクI/Oで劣るけれども割り切って使うにはありだと思います。 今回は英字キーボードが国内の中古市場には見つからなかったのでeBayから輸入しましたが 日本語キーボードでよければ国内の中古市場にも十分流通してるのでそちらの方が早いと思います。 注文 eBayからThinkPad X220を検索しそこそこ状態のよさそうな品かつリース品を綺麗にしたようなものをDyminという業者から購入。 X220にはタッチパネルモデルもあるみたいだけど無駄に重くなるので普通のモデルにした。 配送から届くまで 注文後1日: 業者から発送 4日: eBayのGlobal集荷センターに到着(ここから長い) 5～14日: 海外配送 14～15日: 国内配送 16日: 到着 パーツも結構流通してるっぽいので国内中古市場で買った日本語キーボード版を英字キーボードに換装したほうが早かったかなと思いました。 到着 MacBook Pro届いた pic.twitter.com/A3lKMTUrYT— shufo (@shufo_) November 27, 2016 (ThinkPad X220です) 海外発送の送料込でちょうど3万程でした。 全体的に使用感はほとんどなく綺麗だったけどキーボードには気にならないレベルだけどわずかに使用感あり。 ThinkPadを買うのは初めてですがこの時代のold ThinkPadは雰囲気があっていいですね。個人的にはキーボードは薄い方が好きですがちゃんとした深さがあるので無駄にガチャガチャやりたくなります。 SSD・メモリ換装 SSDとメモリを以下に換装。 パーツ費用含めても全体で大体5万くらいでした。 換装済みの中古品もあるけど高くつく割には容量が少なかったりするので手間をかけられるなら自分で換装した方が安く高スペックに出来ます。 SSD Crusial MX300 525GB 交換手順参照: ThinkPad x220 SSD交換(Apacer/SanDisk) \| 003SH 解体新書 メモリ Transcend PC3L-12800 DDR3L 1600 8GB×2 交換手順参照: ThinkPad X220 増強作戦 その１ ： 1万円でメモリを16GBに増設 - きままテック まとめ MBPの代替としてThinkPad X220を買った。 モバイル出来るUnixとしてMacは好きなのだけど最近のAppleの方向性からロックインが強くなることの懸念も含めてノートPCから変えてみました。(まだiPhoneは使ってる) 最近のWindows開発環境としてはbabun + Docker for Windowsがあれば個人的に困ることはほぼなくなって来たのであとはWSLのbashの完成度がより高まればいよいよWindowsでも十分だなと思います。 Linuxデスクトップはelementary OSが気になっているのでmSATA SSDを追加してデュアルブートにでもしてみようかなと思います。 他候補 MBPオルタナティブのノートPCとしては Razer Blade Stealth Dell XPS 13 がスペックと価格のバランスがよかったので検討していたのだけど自分のノートPCの使用頻度に対してはオーバースペックすぎたので候補から外れました。 追記 2016-12-01 到着後Windows7からWindows10 Proにアップグレードしバッテリーのファームウェアをアップグレードしようとしたらバッテリーが死にました… ファームウェアをアップデートする前はバッテリーを認識していたので恐らくファームウェアの書き換えに失敗したようです。 バッテリー追加 結局以下の互換バッテリーをAmazonでポチりました 元は9セルでX220本体からバッテリーがはみ出していたのが筐体の枠にちょうど収まりました 公式じゃないですけど今のとこ問題ないです 安いし物理的に取り外しし易いからバッテリーが心配な時は何本か持ち歩くってのもありですね WorldPlus バッテリー LENOVO ThinkPad X230 X230i X220 X220i X220s 対応 6セル", 
            "tags": "PC", 
            "loc": "http://shufo.github.io/alternative_to_mac.html"
        },
        {
            "title": "Pelican + codeanywhere + Github Pagesでシンクライアントなブログ作成", 
            "text":"Overview このブログはPelicanで書いているけどいわゆるサイトジェネレータにありがちな問題なのがいちいち開発環境を開いたりするのが面倒というのがある。 WebUI管理画面をサイトジェネレータ自体が持っているHubPressは気軽に記事を投稿出来るもののGithub APIを使うためGithub Pages専用アプリという色が強く他の環境に移す場合を考慮して使ってはいない。 2015年11月現在主なサイトジェネレータはStatic Site GeneratorsでGitHub Starの多い順でJekyll, OctoPress, Hexo, Hugo, Pelicanがよく使われている。 それぞれWebベースの管理画面はないもののシンプルな構成で一度生成してしまえば特定のランタイムやDBに依存しないで静的コンテンツを配信出来る。 やりたいこと 特定のマシン(PC, Mac, タブレット)に依存しないサイトジェネレータ環境の構築 やったこと Pelicanで静的コンテンツの生成、codeanywhereでWebベースでのファイルの編集とサイトジェネレータ用ランタイムの構築、GitHub Pagesでサイトの公開を行った。 Pelican とは PelicanはPython製の静的サイトジェネレータでMarkdown形式で記事を書くことが出来る。 主な機能としては 記事の作成 RSS フィードの作成 テーマ機能 プラグイン があり、ここら辺は類似のサイトジェネレータと特に変わりなし。 Github Pages とは GitHub で静的なページをホスティングすることが出来るサービス。 独自ドメインの割当も出来る。 codeanywhere とは codeanywhereは Web上でコードの編集や実行が出来るクラウドIDE。 プロジェクトごとにコンテナが作成されランタイム等は自由にインストール出来る。 ちなみにこの記事もcodeanywhereで作成から公開まで行っている。 主な機能としては GitHub, BitBucketからのコードの取得・編集 コンテナ内での任意のコマンドの実行 FTPアップロード SSH接続 がある。今回はPython製ツールを使うのでコンテナのテンプレートにPythonを使いプロジェクトを作成してみる。 やってみる まずGitHubで「ユーザ名.github.io」という名前で空のリポジトリを作る。 自分の場合GitHubユーザ名はshufoなのでshufo.github.ioを作成。 作成出来たらcodeanywhereのプロジェクトページから適当に新規プロジェクトを作成する。 先ほど作成したユーザ名.github.ioを選び、コンテナのテンプレートとしてpythonを選択する。 Nextボタンをクリックししばらく待つとプロジェクト編集画面が開きファイルを編集出来るようになるので 左のファイルツリーからレポジトリ名を右クリックし「SSH Terminal」をクリックする。 ターミナルが開くのでPelicanのインストールをする。 sudo pip install pelican Markdown mdx_linkify mdx_del_ins ghp-import 次にサイトの初期化。設定するパラメータを色々質問されるので必要に応じてyまたはnを押して進める。 Please answer the following questions so this script can generate the files needed by Pelican. &gt; Where do you want to create your new web site? [.] &gt; What will be the title of this web site? test &gt; Who will be the author of this web site? shufo &gt; What will be the default language of this web site? [en] ja &gt; Do you want to specify a URL prefix? e.g., http://example.com (Y/n) y &gt; What is your URL prefix? (see above example; no trailing slash) http://shufo.github.io &gt; Do you want to enable article pagination? (Y/n) y &gt; How many articles per page do you want? [10] &gt; What is your time zone? [Europe/Paris] Asia/Tokyo &gt; Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) y &gt; Do you want an auto-reload &amp; simpleHTTP script to assist with theme and site development? (Y/n) y &gt; Do you want to upload your website using FTP? (y/N) n &gt; Do you want to upload your website using SSH? (y/N) n &gt; Do you want to upload your website using Dropbox? (y/N) n &gt; Do you want to upload your website using S3? (y/N) n &gt; Do you want to upload your website using Rackspace Cloud Files? (y/N) n &gt; Do you want to upload your website using GitHub Pages? (y/N) y &gt; Is this your personal page (username.github.io)? (y/N) y Done. Your new project is available at /home/cabox/workspace 初期化が完了したので記事を書いていく。contentディレクトリ以下に拡張子を.mdで保存したファイルが記事の対象となる。 基本的な記事のフォーマットは以下の様になる。 Title: Pelican + codeanywhere + Github Pagesでブラウザのみでブログ構築 Date: 2015-11-23 13:00 Category: Pelican Tags: pelican Slug: site_building Author: shufo Summary: Pelican + codeanywhere + Github Pagesでの非環境依存なブログ構築 ## Pelican とは 以下本文 テーマを適用。テーマは公式のテーマ一覧から適当に。 mkdir pelican-themes git clone https://github.com/lucachr/pelican-mg.git pelican-themes/pelican-mg pelicanconf.pyを編集。 THEME = &#39;./pelican-themes/pelican-mg&#39; 記事の出力とサーバの起動を実行。 ./develop_server.sh start 確認用のサーバURLは画面左のファイルツリーのリポジトリを右クリックし「Info」から表示出来る。 ファイルビューに環境情報が表示される。 http://preview.ys2w6ouag0c0udid1stmdqerhrara4iot2p57l4efhzd7vi.box.codeanywhere.com 先ほど起動したHTTPサーバはデフォルトで8000番でListenするので http://preview.ys2w6ouag0c0udid1stmdqerhrara4iot2p57l4efhzd7vi.box.codeanywhere.com:8000 をブラウザで開いてみる。 確認出来た。 記事に問題がなければ公開を実行。 make github このコマンドを実行するとghp-importでoutputディレクトリの内容がmasterブランチにコミットされ、同時にmasterブランチがGitHubへpushされ公開される。 pushから反映までは10分ほど時間かかるのでしばらく待ち反映されたのを確認出来たら完了。 まとめ PelicanとcodeanywhereとGitHub Pagesで意識低めのブログ構築をした。 ローカルの環境に依存しないためいつでも気が向いたときに書けるというのがブログ作成に当たっては大きいメリットだと思う。", 
            "tags": "Pelican", 
            "loc": "http://shufo.github.io/site_building.html"
        },
        {
            "title": "マルチホストdocker環境でのBlueGreenなデプロイメント", 
            "text":"Overview dockerをそれなりに扱おうと思うと直面するのがマルチホスト環境でのdockerの構成。 大抵シングルホストのプリミティブな環境では問題無かったL3/L4の扱い、IPアドレス、ポート等のメタデータのリソース管理が問題になってくる。 前者に関しては、ルーティングコンテナ経由でのパケット交換、cgroup/namespaced、Open vSwtichなどでSDNを実装、 L3/L4を抽象化し仮想的に１つのネットワークとして扱えるようにすることで解決をしようという動きがある。 代表的なソリューションとしてはsocketplane, weave, pipework, flannel, Open vSwitch等のソリューションがある。 後者に関して分散Key Valueストアにコンテナのメタデータを登録し必要に応じてクラスタの構成情報を読み出す ことで解決しようとする動きがある。 代表的なソリューションとしてはConsul、etcd、zookeeper等がある。 マルチホストdocker環境で辛いところ L3/L4管理 コンテナの配置スケジューリング マルチホストでのメタデータ管理 やりたいこと L3/L4の自動管理 スケジューリングの自動化 コンテナライフサイクル管理の自動化 configのdynamicな書き換え BlueGreenなデプロイメント やったこと weaveでホストごとに存在するdockerのプライベートネットワークをL3レベルで抽象化、複数のホストにまたがるdockerネットワークを一つのネットワークとして扱えるようにし、Consul/consul-template/registratorでメタデータの管理/configの自動書き換え及びBlueGreenなクラスタの切り替え、またコンテナのスケジューリング/ライフサイクル管理にdocker-swarm/docker-composeを使用しクラスタ全体を透過的に管理出来るようにした。 それぞれの構成要素は単体で落ちても他の構成要素には影響しないものとし、ホスト障害があってもクラスタ全体としては可用性を存続出来るようにする。 なお前提として各構成要素は全てコンテナのためOSは便宜的にCoreOSを使用しているがホストでdockerさえ動作すればどんな環境でも動作するようになっている。 構成イメージ Requirements CoreOS (Tested on 557.2.0) Docker (Tested on 1.4.1) Weave (Tested on 0.9.0) docker-swarm (Tested on 0.1.0) docker-compose (Tested on 1.1.0) Consul (Tested on 0.5.0) Consul-template (Tested on 0.7.0) registrator (Tested on v5) Weave dockerネットワークの抽象化にはWeaveを使う。 Weaveは各ホストに存在するローカルなdockerネットワークを抽象化し、１つのネットワークとして扱うことを可能にする。 Unitファイル作成 sudo vim /etc/systemd/system/install-weave.service [Unit] After=network-online.target After=docker.service Description=Install Weave Documentation=http://zettio.github.io/weave/ Requires=network-online.target Requires=docker.service [Service] Type=oneshot RemainAfterExit=yes ExecStartPre=/usr/bin/wget -N -P /opt/bin \ https://raw.github.com/zettio/weave/master/weave ExecStartPre=/usr/bin/chmod +x /opt/bin/weave ExecStartPre=/usr/bin/docker pull zettio/weave:latest ExecStart=/bin/echo Wave Installed weave用ブリッジインターフェースUnitファイル作成 sudo vim /etc/systemd/network/10-weave.network [Match] Type=bridge Name=weave* [Network] Address=10.0.0.1/8 sudo vim /etc/systemd/network/10-weave.netdev [NetDev] Name=weave Kind=bridge weave service用Unitファイル作成 sudo vim /etc/systemd/system/weave.service [Unit] After=install-weave.service Description=Weave Network Documentation=http://zettio.github.io/weave/ Requires=install-weave.service [Service] ExecStartPre=/opt/bin/weave launch ExecStart=/usr/bin/docker attach weave ネットワーク再起動 sudo systemctl restart systemd-networkd weaveインストール・起動 sudo systemctl start install-weave.service sudo systemctl start weave.service sudo /opt/bin/weave create-bridge Docker docker用の管理ポートを開ける。 Unitファイル作成 sudo vim /etc/systemd/system/docker-tcp.socket [Unit] Description=Docker Socket for the API [Socket] ListenStream=2375 BindIPv6Only=both Service=docker.service [Install] WantedBy=sockets.target dockerのデフォルトブリッジをweaveし、dockerから透過的にweaveネットワークを扱えるようにする。 sudo cp /usr/lib/systemd/system/docker.service /etc/systemd/system/ sudo vim /etc/systemd/system/docker.service Environment=DOCKER_OPTS=&#39;--bridge=weave --fixed-cidr=&#34;10.0.0.0/8&#34; --insecure-registry=&#34;0.0.0.0/0&#34;&#39; socketを起動 sudo systemctl enable docker-tcp.socket sudo systemctl stop docker sudo systemctl start docker-tcp.socket sudo systemctl start docker 適宜iptables等でListenする相手を制限。 # Accept a manage node sudo iptables -A INPUT -s 10.0.0.1 -m tcp -p tcp --dport 2375 -j ACCEPT # Drop other nodes sudo iptables -A INPUT -s 0.0.0.0/0 -m tcp -p tcp --dport 2375 -j DROP 参照: https://coreos.com/docs/launching-containers/building/customizing-docker/ Swarm クラスタIDを取得 docker run --rm swarm create 6856663cdefdec325839a4b7e1de38e8 Swarm agentを起動(各ノード) docker run -d --name swarm_agent swarm join --addr=&lt;node_ip:2375&gt; token://&lt;cluster_id&gt; Swarm managerを起動 docker run -d --name swarm_manager -p &lt;swarm_port&gt;:2375 swarm manage token://&lt;cluster_id&gt; 確認 docker run --rm swarm list token://&lt;cluster_id&gt; 終了 docker kill swarm_agent docker kill swarm_manager Compose 実行ファイルをDL curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose.ymlにコンテナの定義を書く mkdir nodes cd nodes vim docker-compose.yml consul: command: --name consul -server -bootstrap -ui-dir /ui -data-dir ./data image: progrium/consul:latest ports: - &#34;8300&#34; - &#34;8400&#34; - &#34;8500&#34; - &#34;8600/udp&#34; environment: - &#34;affinity:container!=nodes_consul_*&#34; net: &#34;host&#34; registrator: command: -internal consul://127.0.0.1:8500 image: sttts/registrator:latest volumes: - &#34;/var/run/docker.sock:/tmp/docker.sock&#34; environment: - &#34;affinity:container!=nodes_registrator_*&#34; net: &#34;host&#34; haproxy: image: shayashibara/docker-consul-template-haproxy environment: - &#34;affinity:container!=nodes_haproxy_*&#34; links: consul:consul ports: - &#34;80:80&#34; apache: image: httpd environment: - &#34;SERVICE_TAGS=production&#34; Swarm経由でComposeを実行 DOCKER_HOST=tcp://localhost:2375 docker-compose scale consul=3 registrator=3 haproxy=3 apache=6 確認 DOCKER_HOST=tcp://localhost:2375 docker-compose ps Consul / consul-template Consulでコンテナのメタデータを管理し、Consul-templateでHAProxy配下のbackendを切り替える。 shayashibara/docker-consul-template-haproxyで起動するhaproxy用のconsul-templateのテンプレート。 global log 127.0.0.1 local0 log 127.0.0.1 local1 notice user haproxy group haproxy defaults log global mode http option httplog option dontlognull balance roundrobin timeout connect 5000 timeout client 50000 timeout server 50000 listen stats bind *:8001 option httpclose option forwardfor stats enable stats uri /haproxy?stats stats auth admin:123123q stats realm HAProxy\ Statistics frontend web-app bind *:80 default_backend {{key &#34;backend/current&#34;}} backend default server s1 localhost:8080 {{range $tag, $services := service &#34;apache-80&#34; | byTag}}backend {{$tag}} balance roundrobin {{range $services}} server {{.ID}} {{.Address}}:{{.Port}} {{end}}{{end}} consulate(Consulのpythonクライアント)で現在のクラスタのタグをsetし切り替える。 要件によるけど任意のタイミングでBlueGreenなクラスタの切り替えを行いたかったのでこのような構成にした。 実作業に落としこむ場合はJenkins等で切り替えをタスク化しWebUIから操作することになると思う。 pip install consulate consulate --api-host 172.17.42.1:8500 kv set backend/current production 確認 docker exec -it nodes_haproxy_1 cat /etc/haproxy/haproxy.cfg クラスタの終了 DOCKER_HOST=tcp://localhost:2375 docker-compose kill DOCKER_HOST=tcp://localhost:2375 docker-compose rm Conclusion マルチホストDocker環境でBlueGreenなデプロイを実現するためweaveでL3の抽象化とConsulによる分散Key-ValueストアでL3/L4リソースの管理、consul-templateで設定ファイルの動的書き換え、docker-swarm/composeによるコンテナのスケジューリング、ライフサイクル管理、BlueGreenなクラスタの切り替えを行った。 構成要素は多いものの各要素は疎結合でそれぞれ代替可能なツールが多い(weaveはpipeworkやflannel、Consulはetcdやzookeeper、consul-templateはconfd等ある)ので、要件に応じて構成要素は変えることが出来る。 dokkuやflynn、deis等のマイクロPaaSやCloud-FoundryやOpenShift等のフルスタックPaaSは単体で上記ツールチェインのほとんどの機能を備えている一方ロックインされやすいという面もあるため今回は既存の技術の組み合わせのみで構成した。 Docker誕生から２年を迎え牧歌的なシングルホストでのdon&#39;t recommend in productionな状態からプロダクションでのマルチホスト環境を見据えたオーケストレーションツール群も大分整理統合されてきた感がある。 依然レポジトリの扱いやセキュリティ面等、実運用面で辛い箇所は残るものの徐々にプロダクション環境での現実的な解が見えてきたように思える。今年はdocker in productionの飛躍の年になることを願いたい。 解決していない課題 プライベートなイメージの扱い OSイメージ + アプリケーションでそれなりの容量になる。 また何をするにもdocker-registry専用のプロトコルが必要なためイメージの配信方法の選択肢が少ない。 ログの扱い 各コンテナが生成するログをどこに送るか。またログ送信の冗長性の担保をどうするか ステートフルなコンテナの扱い 主にRedis, MySQL, PostgreSQLといった状態を持ったアプリケーションをどうするか セキュリティ面 dockerデーモン自体の脆弱性、コンテナの脆弱性、ホストOSの脆弱性をどうするか 課題解決への布石 プライベートなイメージの扱い レポジトリ自体の冗長化、容量コスト等考えるとquay.io, Google Container Registry等のRegistry as a Serviceという選択肢もある。 ログの扱い logsoutput, journalctl, fluentd等 ステートフルなコンテナの扱い ホスト側にマウント、data volume container、またはflocker、そもそもDockerizeしない等 セキュリティ面 Docker層だけで対応するのは難しいのでDockerを乗せるIaaSレベルでのコントロール。 ホスト側をなるべく薄く保つという意味でCoreOSやAtomic host等のコンテナ向け軽量OSやCoreOSの自動アップデート機能を使う。 またCoreOSによるコンテナ実装のRocketからのセキュリティ面でのフィードバックを今後期待。 References Adventures with Weave and Docker docker - ELB+Swarm+Compose+Consul+Registratorで夢は叶うのか(1) - Qiita logspoutでDockerコンテナのログの集約・ルーティング | SOTA", 
            "tags": "Docker", 
            "loc": "http://shufo.github.io/weave_swarm_compose.html"
        },
        {
            "title": "Hello world", 
            "text":"DevとOpsを行ったり来たりしているウェブ系エンジニアのブログです。 Pelicanで生成した静的サイトをGithub Pagesでホスティングしています。 ※このブログでの発言は個人の見解であり、所属する組織の公式の見解ではありません。", 
            "tags": "about", 
            "loc": "http://shufo.github.io/hello-world.html"
        }        
    ]
}